# AStack Chat Server - 后端环境变量配置

# ===== LLM 模型配置 =====
# Deepseek API Key (推荐)
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# OpenAI API Key (可选)
# OPENAI_API_KEY=your_openai_api_key_here

# ===== 服务器配置 =====
# 服务器端口，默认 8080
PORT=8080

# 服务器绑定地址，默认 0.0.0.0
HOST=0.0.0.0

# ===== 日志配置 =====
# 日志级别: trace, debug, info, warn, error, fatal
LOG_LEVEL=info

# ===== CORS 配置 =====
# 允许的前端域名，用逗号分隔
# ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001

# ===== Agent 配置 =====
# 数学 Agent 温度参数（0-1）
MATH_AGENT_TEMPERATURE=0.3

# 文本分析 Agent 温度参数（0-1）
TEXT_AGENT_TEMPERATURE=0.7

# 普通聊天温度参数（0-1）
CHAT_TEMPERATURE=0.7

# ===== 流式性能优化配置 =====
# LLM 最大 token 数量
LLM_MAX_TOKENS=2048

# 流式传输延迟（毫秒）
# 0 = 无延迟（最快）
# 1-5 = 几乎无感知延迟
# 10-20 = 自然打字感觉
# 50+ = 慢速演示效果
STREAMING_DELAY_MS=0

# 流式传输模式
# true = 按字符流式（更细腻但性能稍差）
# false = 按词语流式（推荐，性能更好）
STREAM_BY_CHARACTER=false